{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 11 11:15:49 2019\n",
    "\n",
    "@author: nasir\n",
    "\n",
    "Implements\n",
    "Amplitude Selective Filtering preprocessing step for POS.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import math\n",
    "# import scipy.io\n",
    "# mat = scipy.io.loadmat('c.mat')\n",
    "\n",
    "def ASF(C):\n",
    "    alpha=.002;delta=0.0001\n",
    "    C_=np.dot(inv(np.diag(np.mean(C,1))),C)-1\n",
    "    L = C.shape[1]\n",
    "    F=np.fft.fft(C_)/L\n",
    "    W=delta/(1e-12+np.abs(F[0,:]))\n",
    "    W=W.astype(np.complex)\n",
    "\n",
    "    W[np.abs(F[0,:]) < alpha]= 1\n",
    "\n",
    "    W=np.stack((W,W,W),axis=0)\n",
    "    F_=F*W\n",
    "\n",
    "    C__ = np.dot(np.diag(np.mean(C,1)) ,(np.fft.ifft(F_)+1))\n",
    "\n",
    "    C__ = C__.astype(np.float)\n",
    "    return C__\n",
    "\n",
    "# C = mat['C']\n",
    "# A = ASF(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Implements\n",
    "Color distortion Filtering preprocessing step for POS.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import math\n",
    "#import scipy.io\n",
    "#mat = scipy.io.loadmat('c.mat')\n",
    "\n",
    "def CDF(C, B):\n",
    "    C_ = np.matmul(inv(np.diag(np.mean(C,1))),C)-1\n",
    "    F = np.fft.fft(C_)\n",
    "    S = np.dot(np.expand_dims(np.array([-1/math.sqrt(6), 2/math.sqrt(6), -1/math.sqrt(6)]), axis=0), F)\n",
    "    W = np.real((S* S.conj()) / np.sum((F * F.conj()),0)[None,:])\n",
    "    W[:, 0:B[0]] = 0\n",
    "    W[:, B[1]+1:] = 0\n",
    "\n",
    "    F_ = F * W\n",
    "    iF = (np.fft.ifft(F_) + 1).real\n",
    "    C__ = np.matmul(np.diag(np.mean(C,1)) , iF)\n",
    "    C = C__.astype(np.float)\n",
    "    return C\n",
    "\n",
    "#C = mat['C']\n",
    "#CDF(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "# from unet_models import UNet16, unet11\n",
    "class FaceSegGPU:\n",
    "    def __init__(self, bs, size=256):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = unet11('unet_celeba.pth', pretrained=True).to(self.device)\n",
    "\n",
    "        # self.net = UNet16(pretrained=True).to(self.device)\n",
    "        # self.net.load_state_dict(torch.load('unet16.pth'))\n",
    "        self.net.eval()\n",
    "        sample = Variable(torch.rand(bs,3,size,size).to(self.device))\n",
    "        self.net(sample)\n",
    "        #  = torch.jit.trace(self.net, sample)\n",
    "        print('___init___')\n",
    "\n",
    "    def get_mask(self, images, shape):\n",
    "        # images = Variable(torch.tensor(images, dtype=torch.float,requires_grad=False).to(device=self.device))\n",
    "        pred = self.net(images)\n",
    "        pred= torch.nn.functional.interpolate(pred, size=[shape[1], shape[2]])\n",
    "        pred = pred.squeeze()\n",
    "        mask = (pred > 0.8)\n",
    "        segmentation = mask.cpu().numpy()\n",
    "        return segmentation.astype('float')\n",
    "\n",
    "    def apply_masks(self, frames_transformed, frames):\n",
    "        masks = self.get_mask(frames_transformed, frames.shape)\n",
    "        frames[masks==0] = 0.0\n",
    "        return frames\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_: int, out: int):\n",
    "        super(ConvRelu, self).__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "    link https://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network used\n",
    "            True - encoder pre-trained with VGG11\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.encoder = models.vgg11(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Sequential(self.encoder[0],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv2 = nn.Sequential(self.encoder[3],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            self.encoder[6],\n",
    "            self.relu,\n",
    "            self.encoder[8],\n",
    "            self.relu,\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            self.encoder[11],\n",
    "            self.relu,\n",
    "            self.encoder[13],\n",
    "            self.relu,\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            self.encoder[16],\n",
    "            self.relu,\n",
    "            self.encoder[18],\n",
    "            self.relu,\n",
    "        )\n",
    "\n",
    "        self.center = DecoderBlock(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv=True)\n",
    "        self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv=True)\n",
    "        self.dec4 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 4, is_deconv=True)\n",
    "        self.dec3 = DecoderBlock(256 + num_filters * 4, num_filters * 4 * 2, num_filters * 2, is_deconv=True)\n",
    "        self.dec2 = DecoderBlock(128 + num_filters * 2, num_filters * 2 * 2, num_filters, is_deconv=True)\n",
    "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(self.pool(conv1))\n",
    "        conv3 = self.conv3(self.pool(conv2))\n",
    "        conv4 = self.conv4(self.pool(conv3))\n",
    "        conv5 = self.conv5(self.pool(conv4))\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec1), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec1)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class UNet16(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network used\n",
    "            True - encoder pre-trained with VGG11\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = torchvision.models.vgg16(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder[0],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[2],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv2 = nn.Sequential(self.encoder[5],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[7],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv3 = nn.Sequential(self.encoder[10],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[12],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[14],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv4 = nn.Sequential(self.encoder[17],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[19],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[21],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv5 = nn.Sequential(self.encoder[24],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[26],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[28],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.center = DecoderBlock(512, num_filters * 8 * 2, num_filters * 8)\n",
    "\n",
    "        self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec3 = DecoderBlock(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(128 + num_filters * 2, num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(self.pool(conv1))\n",
    "        conv3 = self.conv3(self.pool(conv2))\n",
    "        conv4 = self.conv4(self.pool(conv3))\n",
    "        conv5 = self.conv5(self.pool(conv4))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec1), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec1)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class DecoderBlockLinkNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # B, C, H, W -> B, C/4, H, W\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
    "\n",
    "        # B, C/4, H, W -> B, C/4, 2 * H, 2 * W\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4,\n",
    "                                          stride=2, padding=1, output_padding=0)\n",
    "        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n",
    "\n",
    "        # B, C/4, H, W -> B, C, H, W\n",
    "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
    "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkNet34(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_channels=3, pretrained=True):\n",
    "        super().__init__()\n",
    "        assert num_channels == 3\n",
    "        self.num_classes = num_classes\n",
    "        filters = [64, 128, 256, 512]\n",
    "        resnet = models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder4 = DecoderBlockLinkNet(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlockLinkNet(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlockLinkNet(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlockLinkNet(filters[0], filters[0])\n",
    "\n",
    "        # Final Classifier\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n",
    "        self.finalrelu1 = nn.ReLU(inplace=True)\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.finalrelu2 = nn.ReLU(inplace=True)\n",
    "        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n",
    "\n",
    "    # noinspection PyCallingNonCallable\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "\n",
    "        # Final Classification\n",
    "        f1 = self.finaldeconv1(d1)\n",
    "        f2 = self.finalrelu1(f1)\n",
    "        f3 = self.finalconv2(f2)\n",
    "        f4 = self.finalrelu2(f3)\n",
    "        f5 = self.finalconv3(f4)\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(f5, dim=1)\n",
    "        else:\n",
    "            x_out = f5\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class Conv3BN(nn.Module):\n",
    "    def __init__(self, in_: int, out: int, bn=False):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.bn = nn.BatchNorm2d(out) if bn else None\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetModule(nn.Module):\n",
    "    def __init__(self, in_: int, out: int):\n",
    "        super().__init__()\n",
    "        self.l1 = Conv3BN(in_, out)\n",
    "        self.l2 = Conv3BN(out, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla UNet.\n",
    "\n",
    "    Implementation from https://github.com/lopuhin/mapillary-vistas-2017/blob/master/unet_models.py\n",
    "    \"\"\"\n",
    "    output_downscaled = 1\n",
    "    module = UNetModule\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_channels: int = 3,\n",
    "                 filters_base: int = 32,\n",
    "                 down_filter_factors=(1, 2, 4, 8, 16),\n",
    "                 up_filter_factors=(1, 2, 4, 8, 16),\n",
    "                 bottom_s=4,\n",
    "                 num_classes=1,\n",
    "                 add_output=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        assert len(down_filter_factors) == len(up_filter_factors)\n",
    "        assert down_filter_factors[-1] == up_filter_factors[-1]\n",
    "        down_filter_sizes = [filters_base * s for s in down_filter_factors]\n",
    "        up_filter_sizes = [filters_base * s for s in up_filter_factors]\n",
    "        self.down, self.up = nn.ModuleList(), nn.ModuleList()\n",
    "        self.down.append(self.module(input_channels, down_filter_sizes[0]))\n",
    "        for prev_i, nf in enumerate(down_filter_sizes[1:]):\n",
    "            self.down.append(self.module(down_filter_sizes[prev_i], nf))\n",
    "        for prev_i, nf in enumerate(up_filter_sizes[1:]):\n",
    "            self.up.append(self.module(\n",
    "                down_filter_sizes[prev_i] + nf, up_filter_sizes[prev_i]))\n",
    "        pool = nn.MaxPool2d(2, 2)\n",
    "        pool_bottom = nn.MaxPool2d(bottom_s, bottom_s)\n",
    "        upsample = nn.Upsample(scale_factor=2)\n",
    "        upsample_bottom = nn.Upsample(scale_factor=bottom_s)\n",
    "        self.downsamplers = [None] + [pool] * (len(self.down) - 1)\n",
    "        self.downsamplers[-1] = pool_bottom\n",
    "        self.upsamplers = [upsample] * len(self.up)\n",
    "        self.upsamplers[-1] = upsample_bottom\n",
    "        self.add_output = add_output\n",
    "        if add_output:\n",
    "            self.conv_final = nn.Conv2d(up_filter_sizes[0], num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = []\n",
    "        for downsample, down in zip(self.downsamplers, self.down):\n",
    "            x_in = x if downsample is None else downsample(xs[-1])\n",
    "            x_out = down(x_in)\n",
    "            xs.append(x_out)\n",
    "\n",
    "        x_out = xs[-1]\n",
    "        for x_skip, upsample, up in reversed(\n",
    "                list(zip(xs[:-1], self.upsamplers, self.up))):\n",
    "            x_out = upsample(x_out)\n",
    "            x_out = up(torch.cat([x_out, x_skip], 1))\n",
    "\n",
    "        if self.add_output:\n",
    "            x_out = self.conv_final(x_out)\n",
    "            if self.num_classes > 1:\n",
    "                x_out = F.log_softmax(x_out, dim=1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class AlbuNet(nn.Module):\n",
    "    \"\"\"\n",
    "        UNet (https://arxiv.org/abs/1505.04597) with Resnet34(https://arxiv.org/abs/1512.03385) encoder\n",
    "        Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with resnet34\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
    "                                   self.encoder.bn1,\n",
    "                                   self.encoder.relu,\n",
    "                                   self.pool)\n",
    "\n",
    "        self.conv2 = self.encoder.layer1\n",
    "\n",
    "        self.conv3 = self.encoder.layer2\n",
    "\n",
    "        self.conv4 = self.encoder.layer3\n",
    "\n",
    "        self.conv5 = self.encoder.layer4\n",
    "\n",
    "        self.center = DecoderBlock(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlock(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlock(128 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlock(64 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2, is_deconv)\n",
    "        self.dec1 = DecoderBlock(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec0 = ConvRelu(num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(dec2)\n",
    "        dec0 = self.dec0(dec1)\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec0), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec0)\n",
    "\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "# from unet_models import UNet16, unet11\n",
    "class FaceSegGPU:\n",
    "    def __init__(self, bs, size=256):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = unet11('unet_celeba.pth', pretrained=True).to(self.device)\n",
    "\n",
    "        # self.net = UNet16(pretrained=True).to(self.device)\n",
    "        # self.net.load_state_dict(torch.load('unet16.pth'))\n",
    "        self.net.eval()\n",
    "        sample = Variable(torch.rand(bs,3,size,size).to(self.device))\n",
    "        self.net(sample)\n",
    "        #  = torch.jit.trace(self.net, sample)\n",
    "        print('___init___')\n",
    "\n",
    "    def get_mask(self, images, shape):\n",
    "        # images = Variable(torch.tensor(images, dtype=torch.float,requires_grad=False).to(device=self.device))\n",
    "        pred = self.net(images)\n",
    "        pred= torch.nn.functional.interpolate(pred, size=[shape[1], shape[2]])\n",
    "        pred = pred.squeeze()\n",
    "        mask = (pred > 0.8)\n",
    "        segmentation = mask.cpu().numpy()\n",
    "        return segmentation.astype('float')\n",
    "\n",
    "    def apply_masks(self, frames_transformed, frames):\n",
    "        masks = self.get_mask(frames_transformed, frames.shape)\n",
    "        frames[masks==0] = 0.0\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import medfilt, decimate\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import pdb\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def scale_pulse(p):\n",
    "    p = p - np.min(p)\n",
    "    p = p/np.max(p)\n",
    "    p-=0.5\n",
    "    p*=2\n",
    "    return p\n",
    "\n",
    "def moving_avg(signal, w_s):\n",
    "    ones = np.ones(w_s) / w_s\n",
    "    moving_avg = np.convolve(signal, ones, 'valid')\n",
    "    return moving_avg\n",
    "\n",
    "def compute_snr(coef):\n",
    "    num_of_bins = len(coef)\n",
    "    max_bin_index = np.argmax(coef)\n",
    "    signal_bins = coef[max_bin_index] + (coef[max_bin_index * 2] if max_bin_index * 2 < num_of_bins else 0)\n",
    "    noise_bins = np.sum(coef) - signal_bins\n",
    "    snr = 20*(np.log10(signal_bins/noise_bins))\n",
    "    return snr\n",
    "\n",
    "def post_process(values, w_s, k_s):\n",
    "    ones = np.ones(w_s) / w_s\n",
    "    moving_avg = np.convolve(values, ones, 'valid')\n",
    "    decimated = decimate(moving_avg, w_s)\n",
    "    filterd =  medfilt(decimated, k_s)\n",
    "    return filterd\n",
    "\n",
    "def compute_mean(frames):\n",
    "    mmm = np.true_divide(frames.sum(axis=(1,2)),(frames!=0).sum(axis=(1,2)))\n",
    "    return mmm\n",
    "\n",
    "def transform_frames(frames, device, size=256):\n",
    "\n",
    "    frames_copy = np.copy(frames)\n",
    "\n",
    "    frames_transposed = np.transpose(frames_copy, (0,3,1,2))\n",
    "\n",
    "    mean = torch.tensor(np.array([0.485, 0.456, 0.406]), dtype=torch.float).to(device=device)\n",
    "    std = torch.tensor(np.array([0.229, 0.224, 0.225]), dtype=torch.float).to(device=device)\n",
    "\n",
    "    tensors = Variable(torch.tensor(frames_transposed, dtype=torch.float).to(device=device)).div(255)\n",
    "\n",
    "    resized = torch.nn.functional.interpolate(tensors, size=(size, size))\n",
    "\n",
    "    normalized = (resized - mean[None, :, None, None]) / std[None, :, None, None]\n",
    "\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_transform(size=256):\n",
    "    t = transforms.Compose([\n",
    "        transforms.Resize(size=(size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    return t\n",
    "\n",
    "def transform_single_frame(frames, size=256):\n",
    "    f = get_transform()\n",
    "    shape = frames.shape\n",
    "\n",
    "    tranformed_frames = np.zeros((shape[0], 3,  shape[1], shape[2]))\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        tranformed_frames[i] = f(Image.fromarray(frames[i]))\n",
    "    return tranformed_frames\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 18 11:15:49 2019\n",
    "\n",
    "@author: nasir\n",
    "\n",
    "Implements\n",
    "Wavelet for post processing of pulse signal\n",
    "\n",
    "\"\"\"\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "import math\n",
    "# import pywt\n",
    "from scipy.signal import convolve2d\n",
    "import pycwt as wavelet\n",
    "\n",
    "class Wavelet():\n",
    "    def __init__(self, sr):\n",
    "        self.samplingRate = sr\n",
    "        self.minFreq = 0.75 #\n",
    "        self.maxFreq = 3 #\n",
    "#\n",
    "    def get_scales(self):\n",
    "        MorletFourierFactor = 4* math.pi / (6+math.sqrt(2+ 6**2))\n",
    "        \"\"\"take default smallest scale (corresponding to Nyquist frequency)\"\"\"\n",
    "        scMin = 1 / (0.5 * self.samplingRate * MorletFourierFactor)\n",
    "        \"\"\"32 scales per octave\"\"\"\n",
    "        ds = 1/32\n",
    "        \"\"\"  we do not consider low freq components \"\"\"\n",
    "        scMax = 1/(0.5 * self.minFreq * MorletFourierFactor)\n",
    "        nSc =  math.floor(math.log2(scMax/scMin) /ds)\n",
    "\n",
    "        scales = scMin * (2 ** (np.arange(0, nSc+1) * ds))\n",
    "        return scales\n",
    "\n",
    "    def get_cwt(self, signal):\n",
    "        scales = self.get_scales()\n",
    "\n",
    "#        \"\"\"get frequencies range of interest 0.67 ~~ 4 Hz\"\"\"\n",
    "        MorletFourierFactor = 4* math.pi / (6+math.sqrt(2+ 6**2))\n",
    "        freqs = 1 / (scales * MorletFourierFactor)\n",
    "\n",
    "        coef, scales, _, coi, fft, fftfreqs = wavelet.cwt(signal, 1/self.samplingRate, wavelet='morlet', freqs=freqs)\n",
    "\n",
    "\n",
    "        firstScaleIndex = np.where(freqs < self.maxFreq)[0][0]\n",
    "        lastScaleIndex = np.where(freqs > self.minFreq)[0][-1]\n",
    "\n",
    "        energyProfile = np.abs(coef)\n",
    "\n",
    "        max_index = np.argmax(energyProfile[firstScaleIndex:lastScaleIndex,:], axis=0)\n",
    "        instantPulseRate = 60 * freqs[firstScaleIndex + max_index]\n",
    "\n",
    "        return coef, instantPulseRate\n",
    "\n",
    "    def get_instant_beats(self, energyProfile):\n",
    "        scales = self.get_scales()\n",
    "        MorletFourierFactor = 4* math.pi / (6+math.sqrt(2+ 6**2))\n",
    "        freqs = 1 / (scales * MorletFourierFactor)\n",
    "        firstScaleIndex = np.where(freqs < self.maxFreq)[0][0]\n",
    "        lastScaleIndex = np.where(freqs < self.minFreq)[0][0]\n",
    "        max_index = np.argmax(energyProfile[firstScaleIndex:lastScaleIndex,:], axis=0)\n",
    "        instantPulseRate = 60 * freqs[firstScaleIndex + max_index - 1]\n",
    "        return instantPulseRate\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cdf import CDF\n",
    "from asf import ASF\n",
    "from numpy.linalg import inv\n",
    "\n",
    "from scipy.fftpack import rfftfreq, rfft\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import pdb\n",
    "from PIL import Image\n",
    "\n",
    "PRE_STEP_ASF = False\n",
    "PRE_STEP_CDF = False\n",
    "\n",
    "class Pulse():\n",
    "    def __init__(self, framerate, signal_size, batch_size, image_size=256):\n",
    "        self.framerate = float(framerate)\n",
    "        self.signal_size = signal_size\n",
    "        self.batch_size = batch_size\n",
    "        self.minFreq = 0.9 #\n",
    "        self.maxFreq = 3 #\n",
    "        self.fft_spec = []\n",
    "\n",
    "    def get_pulse(self, mean_rgb):\n",
    "        seg_t = 3.2\n",
    "        l = int(self.framerate * seg_t)\n",
    "        H = np.zeros(self.signal_size)\n",
    "\n",
    "        B = [int(0.8 // (self.framerate / l)), int(4 // (self.framerate / l))]\n",
    "\n",
    "        for t in range(0, (self.signal_size - l + 1)):\n",
    "            # pre processing steps\n",
    "            C = mean_rgb[t:t+l,:].T\n",
    "\n",
    "            if PRE_STEP_CDF:\n",
    "                C = CDF(C, B)\n",
    "\n",
    "            if PRE_STEP_ASF:\n",
    "                C = ASF(C)\n",
    "\n",
    "            # POS\n",
    "            mean_color = np.mean(C, axis=1)\n",
    "            diag_mean_color = np.diag(mean_color)\n",
    "            diag_mean_color_inv = np.linalg.inv(diag_mean_color)\n",
    "            Cn = np.matmul(diag_mean_color_inv,C)\n",
    "            projection_matrix = np.array([[0,1,-1],[-2,1,1]])\n",
    "            S = np.matmul(projection_matrix,Cn)\n",
    "            std = np.array([1,np.std(S[0,:])/np.std(S[1,:])])\n",
    "            P = np.matmul(std,S)\n",
    "            H[t:t+l] = H[t:t+l] +  (P-np.mean(P))\n",
    "        return H\n",
    "\n",
    "    def get_rfft_hr(self, signal):\n",
    "        signal_size = len(signal)\n",
    "        signal = signal.flatten()\n",
    "        fft_data = np.fft.rfft(signal) # FFT\n",
    "        fft_data = np.abs(fft_data)\n",
    "\n",
    "        freq = np.fft.rfftfreq(signal_size, 1./self.framerate) # Frequency data\n",
    "\n",
    "        inds= np.where((freq < self.minFreq) | (freq > self.maxFreq) )[0]\n",
    "        fft_data[inds] = 0\n",
    "        bps_freq=60.0*freq\n",
    "        max_index = np.argmax(fft_data)\n",
    "        fft_data[max_index] = fft_data[max_index]**2\n",
    "        self.fft_spec.append(fft_data)\n",
    "        HR =  bps_freq[max_index]\n",
    "        return HR\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pulse import Pulse\n",
    "import time\n",
    "from threading import Lock, Thread\n",
    "from plot_cont import DynamicPlot\n",
    "from capture_frames import CaptureFrames\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "\n",
    "\n",
    "class ProcessMasks():\n",
    "\n",
    "    def __init__(self, sz=270, fs=30, bs=30, size=256):\n",
    "        print('init')\n",
    "        self.stop = False\n",
    "        self.masked_batches = []\n",
    "        self.batch_mean = []\n",
    "        self.signal_size = sz\n",
    "        self.batch_size = bs\n",
    "        self.signal = np.zeros((sz, 3))\n",
    "        self.pulse = Pulse(fs, sz, bs, size)\n",
    "        self.hrs = []\n",
    "        self.save_results = True\n",
    "\n",
    "    def __call__(self, pipe, plot_pipe, source):\n",
    "        self.pipe = pipe\n",
    "        self.plot_pipe = plot_pipe\n",
    "        self.source = source\n",
    "        compute_mean_thread =  Thread(target=self.compute_mean)\n",
    "        compute_mean_thread.start()\n",
    "\n",
    "        extract_signal_thread =  Thread(target=self.extract_signal)\n",
    "        extract_signal_thread.start()\n",
    "\n",
    "        self.rec_frames()\n",
    "\n",
    "        compute_mean_thread.join()\n",
    "        extract_signal_thread.join()\n",
    "\n",
    "    def rec_frames(self):\n",
    "        while True and not self.stop:\n",
    "            data = self.pipe.recv()\n",
    "\n",
    "            if data is None:\n",
    "                self.terminate()\n",
    "                break\n",
    "            batch = data[0]\n",
    "\n",
    "            self.masked_batches.append(batch)\n",
    "\n",
    "    def process_signal(self, batch_mean):\n",
    "        size = self.signal.shape[0]\n",
    "        b_size = batch_mean.shape[0]\n",
    "\n",
    "        self.signal[0:size-b_size] = self.signal[b_size:size]\n",
    "        self.signal[size-b_size:] = batch_mean\n",
    "        p = self.pulse.get_pulse(self.signal)\n",
    "        p = moving_avg(p, 6)\n",
    "        hr = self.pulse.get_rfft_hr(p)\n",
    "        if len(self.hrs) > 300: self.hrs.pop(0)\n",
    "\n",
    "        self.hrs.append(hr)\n",
    "        if self.plot_pipe is not None and self.stop:\n",
    "            self.plot_pipe.send(None)\n",
    "        elif self.plot_pipe is not None:\n",
    "            self.plot_pipe.send([p, self.hrs])\n",
    "        else:\n",
    "            hr_fft = moving_avg(self.hrs, 3)[-1] if len(self.hrs) > 5 else self.hrs[-1]\n",
    "            sys.stdout.write(f'\\rHr: {round(hr_fft, 0)}')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def extract_signal(self):\n",
    "        signal_extracted = 0\n",
    "\n",
    "        while True and not self.stop:\n",
    "            if len(self.batch_mean) == 0:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "\n",
    "            mean_dict = self.batch_mean.pop(0)\n",
    "            mean = mean_dict['mean']\n",
    "\n",
    "            if mean_dict['face_detected'] == False:\n",
    "                if self.plot_pipe is not None:\n",
    "                    self.plot_pipe.send('no face detected')\n",
    "                continue\n",
    "            if signal_extracted >= self.signal_size:\n",
    "                self.process_signal(mean)\n",
    "            else:\n",
    "                self.signal[signal_extracted: signal_extracted + mean.shape[0]] = mean\n",
    "            signal_extracted+=mean.shape[0]\n",
    "\n",
    "\n",
    "    def compute_mean(self):\n",
    "        curr_batch_size = 0\n",
    "        batch = None\n",
    "        while True and not self.stop:\n",
    "            if len(self.masked_batches) == 0:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "\n",
    "            mask = self.masked_batches.pop(0)\n",
    "            if batch is None:\n",
    "                batch = np.zeros((self.batch_size, mask.shape[0], mask.shape[1], mask.shape[2]))\n",
    "\n",
    "            if curr_batch_size < (self.batch_size - 1):\n",
    "                batch[curr_batch_size] = mask\n",
    "                curr_batch_size+=1\n",
    "                continue\n",
    "\n",
    "            batch[curr_batch_size] = mask\n",
    "            curr_batch_size = 0\n",
    "\n",
    "            non_zero_pixels = (batch!=0).sum(axis=(1,2))\n",
    "            total_pixels = batch.shape[1] * batch.shape[2]\n",
    "            avg_skin_pixels = non_zero_pixels.mean()\n",
    "            m = {'face_detected': True, 'mean': np.zeros((self.batch_size, 3))}\n",
    "            if (avg_skin_pixels + 1) / (total_pixels) < 0.05:\n",
    "                m['face_detected'] = False\n",
    "            else:\n",
    "                m['mean'] = np.true_divide(batch.sum(axis=(1,2)), non_zero_pixels+1e-6)\n",
    "\n",
    "            self.batch_mean.append(m)\n",
    "\n",
    "    def terminate(self):\n",
    "\n",
    "        if self.plot_pipe is not None:\n",
    "            self.plot_pipe.send(None)\n",
    "        self.savePlot(self.source)\n",
    "        self.saveresults()\n",
    "        self.stop = True\n",
    "\n",
    "    def saveresults(self):\n",
    "        \"\"\"\n",
    "        saves numpy array of heart rates as hrs\n",
    "        saves numpy array of power spectrum as fft_spec\n",
    "        \"\"\"\n",
    "        np.save('hrs', np.array(self.hrs))\n",
    "        np.save('fft_spec', np.array(self.pulse.fft_spec))\n",
    "\n",
    "    def savePlot(self, path):\n",
    "        if self.save_results == False:\n",
    "            return\n",
    "\n",
    "        # path = path.replace   ('/media/munawar/','/munawar-desktop/')\n",
    "        # fig_path = path[40:].replace(\"/\",\"_\")\n",
    "\n",
    "        # file_path = path.replace('video.avi','gt_HR.csv')\n",
    "        # gt_HR = pd.read_csv(file_path, index_col=False).values\n",
    "        if len(self.hrs) == 0:\n",
    "            return\n",
    "\n",
    "        ax1 = plt.subplot(1,1,1)\n",
    "        ax1.set_title('HR')\n",
    "        ax1.set_ylim([20, 180])\n",
    "        ax1.plot(moving_avg(self.hrs, 6))\n",
    "\n",
    "        # ax3 = plt.subplot(1,2,2)\n",
    "        # ax3.set_title('GT')\n",
    "        # ax3.set_ylim([20, 180])\n",
    "        # ax3.plot(gt_HR[8:])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from scipy.signal import medfilt, decimate\n",
    "\n",
    "plt.ion()\n",
    "class DynamicPlot():\n",
    "    def __init__(self, signal_size, bs):\n",
    "        self.batch_size = bs\n",
    "        self.signal_size = signal_size\n",
    "        self.launched = False\n",
    "\n",
    "    def launch_fig(self):\n",
    "        self.fig, (self.pulse_ax, self.hr_axis)= plt.subplots(2, 1)\n",
    "\n",
    "        self.pulse_to_plot = np.zeros(self.signal_size)\n",
    "        self.hrs_to_plot = np.zeros(self.signal_size)\n",
    "        # self.rgb_to_plot = np.zeros(self.signal_size)\n",
    "\n",
    "        self.hr_texts = self.pulse_ax.text(0.1, 0.9,'0', ha='center', va='center', transform=self.pulse_ax.transAxes)\n",
    "        self.pulse_ax.set_title('BVP')\n",
    "        self.hr_axis.set_title('Heart Rate')\n",
    "        # self.rgb_axis.set_title('color')\n",
    "        self.pulse_ax.set_autoscaley_on(True)\n",
    "\n",
    "        self.pulse_ax.plot(self.pulse_to_plot)\n",
    "        self.hr_axis.plot(self.hrs_to_plot)\n",
    "        # self.rgb_axis.plot(self.rgb_to_plot)\n",
    "\n",
    "        self.pulse_ax.set_ylim(-3,3)\n",
    "        self.hr_axis.set_ylim(0,180)\n",
    "        self.launched = True\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def __call__(self, pipe):\n",
    "        if self.launched == False: self.launch_fig()\n",
    "        self.pipe = pipe\n",
    "        self.call_back()\n",
    "\n",
    "    def call_back(self):\n",
    "        while True:\n",
    "            data = self.pipe.recv()\n",
    "            if data is None:\n",
    "                self.terminate()\n",
    "                break\n",
    "            elif data == 'no face detected':\n",
    "                self.update_no_face()\n",
    "            else:\n",
    "                self.update_data(data[0], data[1])\n",
    "\n",
    "    def update_no_face(self):\n",
    "        hr_text = 'HR: NaN'\n",
    "        self.hr_texts.set_text(hr_text)\n",
    "\n",
    "        scaled = np.zeros(10)\n",
    "        for i in range(0, len(scaled)):\n",
    "            self.pulse_to_plot[0:self.signal_size-1] = self.pulse_to_plot[1:]\n",
    "            self.pulse_to_plot[-1] = scaled[i]\n",
    "            self.update_plot(self.pulse_ax, self.pulse_to_plot)\n",
    "\n",
    "            self.hrs_to_plot[0:self.signal_size-1] = self.hrs_to_plot[1:]\n",
    "            self.hrs_to_plot[-1] = 0\n",
    "            self.update_plot(self.hr_axis, self.hrs_to_plot)\n",
    "            self.re_draw()\n",
    "\n",
    "    def update_data(self, p, hrs):\n",
    "\n",
    "        hr_fft = moving_avg(hrs, 3)[-1] if len(hrs) > 5 else hrs[-1]\n",
    "        hr_text = 'HR: ' + str(int(hr_fft))\n",
    "        self.hr_texts.set_text(hr_text)\n",
    "\n",
    "        # ma = moving_avg(p[-self.batch_size:], 6)\n",
    "        batch = p[-self.batch_size:]\n",
    "        decimated_p = decimate(batch, 3)\n",
    "        # filterd_p =  medfilt(decimated_p, 5)\n",
    "        scaled = scale_pulse(decimated_p)\n",
    "        for i in range(0, len(scaled)):\n",
    "            self.pulse_to_plot[0:self.signal_size-1] = self.pulse_to_plot[1:]\n",
    "            self.pulse_to_plot[-1] = scaled[i]\n",
    "            self.update_plot(self.pulse_ax, self.pulse_to_plot)\n",
    "            self.hrs_to_plot[0:self.signal_size-1] = self.hrs_to_plot[1:]\n",
    "            self.hrs_to_plot[-1] = hr_fft\n",
    "            self.update_plot(self.hr_axis, self.hrs_to_plot)\n",
    "            self.re_draw()\n",
    "\n",
    "\n",
    "    def update_plot(self, axis, y_values):\n",
    "        line = axis.lines[0]\n",
    "        line.set_xdata(np.arange(len(y_values)))\n",
    "        line.set_ydata(y_values)\n",
    "        axis.relim()\n",
    "        axis.autoscale_view()\n",
    "    def re_draw(self):\n",
    "        #print(self.fig.canvas.tostring_rgb())\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "\n",
    "\n",
    "    def terminate(self):\n",
    "        \"\"\"\n",
    "        saves numpy array of rPPG signal as pulse\n",
    "        \"\"\"\n",
    "        np.save('pulse', self.pulse_to_plot)\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-13767828-9d5f-4c1a-a240-a302f5a8367d.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11588\\698366594.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[0msource\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[0mrunPOS\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mRunPOS\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m270\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframerate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatchsize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m     \u001B[0mrunPOS\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11588\\698366594.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, source)\u001B[0m\n\u001B[0;32m     31\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot_process\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 33\u001B[1;33m         \u001B[0mprocess_mask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mProcessMasks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignal_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframe_rate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m         \u001B[0mmask_processer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mProcess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprocess_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchil_process_pipe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot_pipe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdaemon\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\rPPG\\process_mask.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, sz, fs, bs, size)\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignal\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpulse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPulse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhrs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_results\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\rPPG\\pulse.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, framerate, signal_size, batch_size, image_size)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mPulse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframerate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msignal_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m256\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframerate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframerate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignal_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msignal_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-13767828-9d5f-4c1a-a240-a302f5a8367d.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pulse import Pulse\n",
    "import time\n",
    "from threading import Lock, Thread\n",
    "from plot_cont import DynamicPlot\n",
    "from capture_frames import CaptureFrames\n",
    "from process_mask import ProcessMasks\n",
    "\n",
    "from utils import *\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "class RunPOS():\n",
    "    def __init__(self,  sz=270, fs=28, bs=30, plot=False):\n",
    "        self.batch_size = bs\n",
    "        self.frame_rate = fs\n",
    "        self.signal_size = sz\n",
    "        self.plot = plot\n",
    "\n",
    "    def __call__(self, source):\n",
    "        time1=time.time()\n",
    "\n",
    "        mask_process_pipe, chil_process_pipe = mp.Pipe()\n",
    "        self.plot_pipe = None\n",
    "        if self.plot:\n",
    "            self.plot_pipe, plotter_pipe = mp.Pipe()\n",
    "            self.plotter = DynamicPlot(self.signal_size, self.batch_size)\n",
    "            self.plot_process = mp.Process(target=self.plotter, args=(plotter_pipe,), daemon=True)\n",
    "            self.plot_process.start()\n",
    "\n",
    "        process_mask = ProcessMasks(self.signal_size, self.frame_rate, self.batch_size)\n",
    "\n",
    "        mask_processer = mp.Process(target=process_mask, args=(chil_process_pipe, self.plot_pipe, source, ), daemon=True)\n",
    "        mask_processer.start()\n",
    "\n",
    "        capture = CaptureFrames(self.batch_size, source, show_mask=True)\n",
    "        capture(mask_process_pipe, source)\n",
    "\n",
    "\n",
    "        mask_processer.join()\n",
    "        if self.plot:\n",
    "            self.plot_process.join()\n",
    "        time2=time.time()\n",
    "        time2=time.time()\n",
    "        print(f'time {time2-time1}')\n",
    "\n",
    "def get_args():\n",
    "    parser = OptionParser()\n",
    "    parser.add_option('-s', '--source', dest='source', default=0,\n",
    "                        help='Signal Source: 0 for webcam or file path')\n",
    "    parser.add_option('-b', '--batch-size', dest='batchsize', default=30,\n",
    "                        type='int', help='batch size')\n",
    "    parser.add_option('-f', '--frame-rate', dest='framerate', default=25,\n",
    "                        help='Frame Rate')\n",
    "\n",
    "    (options, _) = parser.parse_args()\n",
    "    return options\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    args = get_args()\n",
    "    source = args.source\n",
    "    runPOS = RunPOS(270, args.framerate, args.batchsize, True)\n",
    "    runPOS(source)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
